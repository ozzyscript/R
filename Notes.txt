

 ======= Thinking & Todo: =======
1 Define all gestures [Done] 18 Gestures in total.

2 Build a confirm mechanism to detect & filter the gesture correctly. [Done]

3 Build a func to optionally show points. [Done]

4 build a mechanism to stop continuous action execution. [Done]

5 Map gestures to actions [Done]

6 Fine tune the model to add more gestures [Done]

7 Document training file [Done]

8 Document main file [Done]

9 Write readme file [Done]

10 Organize Notes file [Done]

11 Add second hand [Done]
    Note: adds allowed to use the left hand as modifier,
    so you can make more than 300 combo gestures.

12 make list of actions/things to happen when a certain gesture detected [Done]

13 fix mouse movement and clicks [Done] 

14 Check and clean all the file [Done]

15 divide the main.py file to modules [Done]

16 create "all_in_one.py" file 

17 implement pause mechanism [Done]





This is version one(Old)

======== KEEP IN MIND ======

1 the resources that talk, guide or explain MediaPipe, uses the older version
which will not work out of the box if you install using pip install.

2 the code in this project uses both, the main.py uses the new version, while the train.py
uses the older version.

3 make sure to use separated virtual environment, one for the project second for training. 
this will help to avoid any compatibility issues.

- for main install mediapipe and opencv
- for training install mediapipe_model_maker 

4 you can use landmarks coordinates to create advanced gestures like, scrolling, or zooming.


======== Starting Point =======

Use this link below to get the data from Kaggle (Thanks to them).
https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p/data


NOTES: 

DIR = FOLDER :)

1 you only need the dir that contain sub folders with images
2 it's recommended to rename the sub dirs so model can use them as labels (gesture name). 
example: dir name = ok. dir name = like
3 the dataset does not include the none dir which is so important for model to not hallucinate.

Use the "download.py" file as shown below to get the "none" dir
=> python3 download.py -d -t no_gesture 

NOTES:
1 rename the dir to "none" then add it with the dataset dirs
2 check for any hidden dirs or files and delete them. It include (my case)
3 the "download.py" I get it from the link below (Thanks to them)
https://github.com/hukenovs/hagrid/blob/master/download.py



======= FINE TUNE THE MODEL (Training). =======

Based on the Internet Search
To fine-tune a MediaPipe model using Model Maker, you should aim for approximately 100 data
samples per class you want the model to recognize.

How to train Docs:
https://ai.google.dev/edge/mediapipe/solutions/customization/gesture_recognizer


NOTES:

1 the "training.py" file is ready for training, so you don't need to do a lot. BUT:
    1 the file is only for one hand
    2 you need to get the dataset, explained how above.
    3 I did that locally on my device, so the code was written for that.

2 if your pc/laptop is not capable enough use google Colab.

3 dir names will be used as labels, so name them carefully. 
    label is the "gesture name" 
    example: dir name = ok means gesture name is fist

4 The python version used for training is 3.10.13
if your system has a newer version, or even an older. It's recommended 
to use pyenv and install this version, to avoid compatibility issues.

5 the only lib you need to install is "mediapipe_model_maker"
it will download, all necessary libs/framworks (size is decent) 
version used for this project is 0.2.1.4.  

6 when you launch training on your device do not panic if you see a bench of warnings 
tanserflow will use CPU if an issue related GPU



How to install a specific version of python (arch my case):
by doing the steps below you can have multiple versions of python 

1 Install dependencies for building Python
sudo pacman -S --needed base-devel libffi zlib xz bzip2 sqlite tk

2 Install pyenv
git clone https://github.com/pyenv/pyenv.git ~/.pyenv

3 Add pyenv to shell
echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.bashrc
echo 'export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.bashrc
echo 'eval "$(pyenv init --path)"' >> ~/.bashrc
source ~/.bashrc

3 Install Python 3.10.13 (latest 3.10)
pyenv install 3.10.13
pyenv global 3.10.13  # or local 3.10.13 per project 

I recommended to install locally (per project)
run the command "pyenv 3.10.13" inside the prj dir

========  links ======= 
Open Cv official docs
https://docs.opencv.org/4.12.0/dd/d43/tutorial_py_video_display.html

MediaPipe
https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer/python#video
https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker
https://ai.google.dev/edge/mediapipe/solutions/customization/gesture_recognizer


====== Challenges =======
To be honest he whole project was a challenge, because this the first time,
I deal with, OpenCv and MediaPipe. This is my first time I fine-tune.
The good thing, I have learned a lot from this project more than any else project.

Note: 
1 Ai (ChatGPT and Perplexity ) helped me a lot, especially ChatGPT
2 I do not copy paste code, I try to understand every line, then write it.



======== While I was building =======
Problems:
Gesture did not pass 85% test:
four
like (needs a specific way)
palm
stop_inverted
appeard suddenly [ three2, stop,two_up,
Note: pass  and three2 passed successfully



Search Reslut:
To fine-tune a MediaPipe model using Model Maker, you should aim for approximately 100 data samples per class you want the model to recognize.
 This approach leverages transfer learning, which allows for effective retraining with significantly less data compared to training a new model from scratch.

Todo:
train the model to work with 2 hands
try to make motions gesture (scroll as start)
Org notes.txt file




[Helper]
draw_landmarks 
filter_gesture
extract_hand_gestures
toggle_capslock
 press_func_key
type_char
 hypr


=================
[Commands]
GESTURE_COMMANDS
COMBO_COMMANDS

[Mouse]
left_click
right_click
middle_click
scroll_up
scroll_down
===============



keyboard [Done]
mouse [Done]
helper [Done]
consts [Done]
system commands


gestures
mouse => ok
keyboard letters => stop + stop inverted



